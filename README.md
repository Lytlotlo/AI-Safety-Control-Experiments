# AI-Safety-control-experiments
## About this Repository

This is my personal lab for building, testing, and learning about AI Control Systems.

AI is advancing rapidly — and while it brings many benefits, it also raises important safety challenges. My focus is on exploring practical ways to help humans stay in control of powerful AI systems.

This repository documents my experiments, projects, and reflections as I learn how to design control systems that can monitor, steer, and intervene in AI behavior.

I believe learning happens best through building — making mistakes, testing ideas, and improving along the way.

---

## Current Experiments

| Project | Description | Status |
|---------|-------------|--------|
| Output Monitor v0.1 | A simple Python script to log and flag potentially dangerous or power-seeking outputs from language models. | In Progress |
| Prompt Layer Control | Experiments with different prompt engineering strategies for influencing model behavior. | Planned |
| Human-in-the-Loop Monitor | A prototype system for allowing human review and override of AI outputs before they are deployed. | Planned |

---

## Learnings & Reflections

All of my notes, lessons, and reflections from building these projects are shared openly here: [learnings/](learnings/)

I document:
- What I tried
- What worked
- What failed
- What surprised me
- What I want to improve next

---

## Why This Matters

AI Safety isn't just about detecting problems — it's about building systems that prevent them.

This repository is a small step towards understanding what control mechanisms could look like in practice.

---

